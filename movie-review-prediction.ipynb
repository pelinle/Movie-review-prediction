{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"  # About Dataset\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n\ntrain.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n\ntest.tsv contains just phrases. You must assign a sentiment label to each phrase.\n\nThe sentiment labels are:\n\n0 - negative\n\n1 - somewhat negative\n\n2 - neutral\n\n3 - somewhat positive\n\n4 - positive","metadata":{"_uuid":"0fdb1f05ab368527aab2a6d69fe4e663de1beff0"}},{"cell_type":"markdown","source":"## Loading important Libraries","metadata":{"_uuid":"6f180c750a0d060cfc94e52c3771e6d67644913f"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n\n\n\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n# statistical natural language processing for English written in the Python programming language.\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom bs4 import BeautifulSoup\nimport re\n\n#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\nfrom tqdm import tqdm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is Keras\nKeras is a deep learning framework that actually under the hood uses other deep learning frameworks in order to expose a beautiful, simple to use and fun to work with, high-level API. Keras can use either of these backends:\n\n\n    Tensorflow – Google’s deeplearning library\n    Theano – may not be further developed\n    CNTK – Microsoft’s deeplearning library\n    MXNet – deeplearning library from Apache.org (currently under development)\n\nKeras uses these frameworks to deliver powerful computation while exposing a beautiful and intuitive (that kinda looks like scikit-learn) API.\n\nHere’s what Keras brings to the table:\n\n    The integration with the various backends is seamless\n    Run training on either CPU/GPU\n    Comes in two flavours: sequential or functional. Just to ways of thinking about building models. The resulting models are perfectly equivalent. We’re going to use the sequential one.\n    Fast prototyping – With all these good abstractions in place, you can just focus more on the problem and hyperparameter tunning.\n    \nLet’s now start using Keras to develop various types of models for Natural Language Processing. Here’s what we’ll be building:\n\n    (Dense) Deep Neural Network – The NN classic model – uses the BOW model\n    Convolutional Network – build a network using 1D Conv Layers – uses word vectors\n    Recurrent Networks – LSTM Network – Long Short-Term Memory – uses word vectors\n    Transfer learning for NLP – Learn how to load spaCy’s vectors or GloVe vectors – uses word vectors","metadata":{}},{"cell_type":"code","source":"from keras.utils import to_categorical\nimport random\nfrom tensorflow import set_random_seed\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\n\n#set random seed for the session and also for tensorflow that runs in background for keras\nset_random_seed(123)\nrandom.seed(123)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{"_uuid":"2f03ae0af5d73a29039f39632d1051ab6de4f3fb"}},{"cell_type":"code","source":"\ntrain= pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\n\ntrain.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"_uuid":"fb1464a9006164fe78aa10d694328dc24159394c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"_uuid":"732aa6143c98b3c2aaf3a95fe8d3c73920485f53","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function for cleaning the reviews, tokenize and lemmatize them.\n\nThis function will take each phrase iteratively and it will \n    \n        remove html content\n        remove non-alphabetic characters\n        tokenize the sentences\n        lemmatize each word to its lemma\nand then return the result in the list named reviews","metadata":{"_uuid":"47d8b13e01fa17aae32be74d598939403b2cfebb"}},{"cell_type":"code","source":"\ndef clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['Phrase']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #lemmatize each word to its lemma\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(lemma_words)\n\n    return(reviews)\n\n","metadata":{"_uuid":"5e1fdc5dbea7aea92a2872c746aea9b25788aad5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cleaned reviews for both train and test set retrieved\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\nprint(len(train_sentences))\nprint(len(test_sentences))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collect the dependent values and convert to one-hot encoded output using to_categorical","metadata":{"_uuid":"cdedece358c942dbca3baf8e38bb8a850e88b2ba"}},{"cell_type":"code","source":"target=train.Sentiment.values\ny_target=to_categorical(target)\nnum_classes=y_target.shape[1]","metadata":{"_uuid":"31e462eadcf747d4f37a5e6c2bc4965d39ce10bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## split into train and validation sets.","metadata":{"_uuid":"16fbf94281d219307165e3195eb799360cc40a53"}},{"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)","metadata":{"_uuid":"33db4ea234f1f76a98f2c0dbe7d9b9938a1fe2d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Geting the no of unique words and max length of a review available in the list of cleaned reviews.","metadata":{"_uuid":"ef359e011decf1ed1f285470d9b3fa1411d55e35"}},{"cell_type":"code","source":"#It is needed for initializing tokenizer of keras and subsequent padding\n\nunique_words = set()\nlen_max = 0\n\nfor sent in tqdm(X_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \n#length of the list of unique_words gives the no of unique words\nprint(len(list(unique_words)))\nprint(len_max)","metadata":{"_uuid":"c634680111a84642b151e862040b0475520d5010","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Actual tokenizer of keras and convert to sequences","metadata":{"_uuid":"7dfc49aeb99251a822a39fed9a8fae0171deb662"}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(X_train))\n\n#texts_to_sequences(texts)\n\n    # Arguments- texts: list of texts to turn to sequences.\n    #Return: list of sequences (one per text input).\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(test_sentences)\n\n#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n\nX_train = sequence.pad_sequences(X_train, maxlen=len_max)\nX_val = sequence.pad_sequences(X_val, maxlen=len_max)\nX_test = sequence.pad_sequences(X_test, maxlen=len_max)\n\nprint(X_train.shape,X_val.shape,X_test.shape)","metadata":{"_uuid":"6f106b6e59545f8d0d4db272acf32b2337e1ad7b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Early stopping to prevent overfitting\nEarly stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models.","metadata":{"_uuid":"f99132a978d0cfa245defb35edfbfc5e8f6151c6"}},{"cell_type":"code","source":"early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]\n\n","metadata":{"_uuid":"27f0a1c26bee93cd660a69df8b79c61094f61abf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model using Keras LSTM\n\n#Multilayer Perceptron (MLP) for multi-class softmax classification:\n#Let’s build what’s probably the most popular type of model in NLP at the moment: Long Short Term Memory network. \n#This architecture is specially designed to work on sequence data.\n#It fits perfectly for many NLP tasks like tagging and text classification.\n#It treats the text as a sequence rather than a bag of words or as ngrams.\n\n#Here’s a possible model definition:\n\nmodel=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## fit the model","metadata":{"_uuid":"d323fed6b9c2ffaeb8b3f71cacb3ff458efe0bf9"}},{"cell_type":"code","source":"#This is done for learning purpose only. One can play around with different hyper parameters combinations\n#and try increase the accuracy even more. For example, a different learning rate, an extra dense layer \n# before output layer, etc. Cross validation could be used to evaluate the model and grid search \n# further to find unique combination of parameters that give maximum accuracy. This model has a validation\n#accuracy of around 66.5%\nhistory=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)","metadata":{"_uuid":"2b1bf946e278f0a9955356351907d512f5018582","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create count of the number of epochs\nepoch_count = range(1, len(history.history['loss']) + 1)\n\n# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n#As mentioned before, altering different hyper parameters especially learning rate can have a positive impact\n#on accuracy and learning curve.\nplt.plot(epoch_count, history.history['loss'], 'r--')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"_uuid":"1524533e318bcdea0e4d3abebbf5b02c6eb49b2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"_uuid":"d9356081721990b2af9de625c7fcad7c241ee61a"}},{"cell_type":"code","source":"#make the predictions with trained model and submit the predictions.\ny_pred=model.predict_classes(X_test)\n\nsub_file = pd.read_csv('../input/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=y_pred\nsub_file.to_csv('Submission.csv',index=False)","metadata":{"_uuid":"03b12920b45234667f67adeedb5596e93a4d3ddd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}